"""
Authors: Colin Rioux, Vincent Zhou

probid - unique problems (source)
subid - unique interpretations (pseudocode)
"""
from typing import Dict, List, Tuple

import glob
import pandas
import pytextrank
import spacy
import argparse

arg_parser = argparse.ArgumentParser(description='Execute iticse modified work')
arg_parser.add_argument('--data_path', help='The location of the input data', default='../data/django')
arg_parser.add_argument('--out_path', help='The location of the output data', default='../result/')
args = arg_parser.parse_args()

INPUT_DIR = args.data_path
OUTPUT_DIR = args.out_path

nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("textrank")

def load_data(file_name) -> Dict[str, Dict[int, List[pandas.Series]]]:
    """
    Creates a dictionary that is first blocked by problem id
    and then by sub id, which is mapped to a list of pandas.Series
    that represent rows in the input data.
    :param file_name: Input file's path.
    :return: Blocked data.
    """
    df = pandas.read_csv(file_name, sep="\t")
    data = {}

    for index, row in df.iterrows():
        prob_id, sub_id = row['probid'], int(row['subid'])
        if prob_id not in data:
            data[prob_id] = {}

        if sub_id not in data[prob_id]:
            data[prob_id][sub_id] = []

        data[prob_id][sub_id].append(row)

    return data


def walk_data(data) -> Dict[int, Dict[int, str]]:
    """
    Calls process_level() on each block organized by indent,
    even if it is not continuous.
    :param data: Preprocessed data generated by load_data()
    :return: A mapping of sub problem id to their generated summaries.
    """
    summaries = {}

    for prob_id, problem in data.items():
        for sub_id, rows in problem.items():
            processed = {}
            index = 0

            while True:
                current = []

                for row in rows:
                    text, indent = row['text'], int(row['indent'])

                    if indent == index:
                        current.append((text, indent))

                if not current:
                    break

                if summary := process_level(current, index):
                    processed[index] = summary
                index += 1

            summaries[sub_id] = processed

    return summaries


def process_level(level: List[Tuple[str, int]], actual):
    """
    Processes an indent level of text.
    :param level: A list of tuples that contain the text and indent, respectively.
    :param actual: The base indent of this level.
    :return: The processed summary.
    """
    joined = ' '.join([f"{text}." if indent == actual else str(text)
                       for text, indent in level if pandas.notnull(text)])

    if not joined:
        return None

    doc = nlp(joined)
    tr = doc._.textrank
    summary = tr.summary(limit_sentences=1)

    return ' '.join(str(sentence) for sentence in summary)


def output_data(file, data, summaries):
    """
    Writes the data to output file.
    """
    output = {"workerid": [], "probid": [], "subid": [], "indent": [], "summary": []}

    for prob_id, problem in data.items():
        for sub_id, sub in problem.items():
            for indent, summary in summaries[sub_id].items():
                worker_id = sub[0]['workerid']
                output["workerid"].append(worker_id)
                output["probid"].append(prob_id)
                output["subid"].append(sub_id)
                output["indent"].append(indent)
                output["summary"].append(summary)

    df = pandas.DataFrame(output)
    df.to_csv(file, sep="\t")


if __name__ == "__main__":
    files = glob.glob(r"./data/unique/*.tsv")
    for file in files:
        fname = file.split('/')[-1].split(',')[0]
        dataset = load_data(file)
        result = walk_data(dataset)
        output_data(str('./data/output/out-') + fname, dataset, result)
